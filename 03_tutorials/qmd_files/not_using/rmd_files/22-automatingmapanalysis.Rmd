# Automating geographic analysis

One thing that has been very apparent with the coronavirus outbreak is that this is a very geographic story. Where cases are being found and how fast is news, so it would be a good idea for us to have updating maps. But to have that, we need to have updating data.

Good news.

The New York Times is making the data behind [their interactive trackers](https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html) [available to others for free](https://github.com/nytimes/covid-19-data).

So we have a constantly updating data stream on Github, so that means we can make this work.

Let's get our libraries first:

```{r}
library(tidyverse)
library(sf)
```

We can use `read_csv` to read a URL if that URL is to a csv file. And Github just happens to provide a direct link to the CSV of county COVID-19 reports. Here's what that looks like:

```{r}
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")
```

Let's look at what the New York Times is providing us:

```{r}
head(covid)
```

If you look, we have a county and a date -- how many cases are reported in that county on that day. That means we can do some interesting progression charts.

Let's filter out Maryland first.

```{r}
maryland <- covid %>% filter(state == "Maryland")
```

And we can create line chart like this:

```{r}
ggplot() + geom_line(data=maryland, aes(x=date, y=cases, group=county, color=county))
```

The colors can be a little hard to make out, but when you have counties with a large number of cases, you'd use a different kind of scale called a log scale. It's a way of representing a very wide range of data in a more compact way (more details here: https://en.wikipedia.org/wiki/Logarithmic_scale). [YOU REALLY SHOULD WATCH THIS](https://www.ft.com/video/9a72a9d4-8db1-4615-8333-4b73ae3ddff8). You've no doubt seen the Financial Times coronavirus trajectory tracker. Hear why they are using a log scale. And here's what our chart looks like with it. Note the y-axis scale.

```{r}
ggplot() + geom_line(data=maryland, aes(x=date, y=cases, group=county, color=county)) + scale_y_continuous(trans="log10")
```

This presents a more consistent story of the increase in cases.

## Mapping continuously

But for a map, we can't have multiple days. We need a single day. Ideally, it would be the most recent date. We can get it using the `max` function.

```{r}
current <- maryland %>% summarize(max(date))
```

That will give us the most recent date in Maryland in a variable called `current`. And now we can filter the most recent data for Nebraska, regardless of when this runs.

I'm adding one piece to the end to make joining this to a map easier and just renaming fips to GEOID, because they are identical in both datasets and can be used for joining.

```{r}
marylandcurrent <- maryland %>% filter(date == current[[1]]) %>% rename(GEOID = fips)
```

Now we can read in our counties map layer.

```{r}
counties <- st_read("data/cb_2018_us_county_5m/cb_2018_us_county_5m.shp")
```

And join the two together.

```{r}
counties <- counties %>% left_join(marylandcurrent)
```

Since we have every county in the United States in our counties map layer, we can filter just Nebraska like this:

```{r}
mdcounties <- counties %>% filter(STATEFP == 24)
```

So now, we have a geographic dataframe that has both the county shapes and the number of cases in the most recent data update. We just need to see it now:

```{r}
ggplot() +
  geom_sf(data=mdcounties, aes(fill=cases)) +
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  theme_void() +
  labs(title = paste("COVID-19 cases as of ", current[[1]], sep=""))
```

As it stands, we can run this every day and get an up-to-date map.
