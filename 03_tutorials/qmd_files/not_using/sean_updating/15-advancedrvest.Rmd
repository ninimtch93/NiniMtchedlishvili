---
output:
  html_document: default
  pdf_document: default
---
# Advanced rvest

In the last chapter, we demonstrated a fairly straightforward example of web scraping to grab a list of NAICS industry sector codes from the BLS website.  

We're going to graduate to a more challenging example, one that will help us gather information about the number of employees in each industry sector. 

What makes this more challenging?  Well, the information we need is all contained on multiple pages, one page per sector. We need to write code to visit each page, and then merge them into a single data frame. 

This is challenging stuff, so don't feel dissuaded if it all doesn't click the first time through.  Like many things, web scraping is something that gets easier with lots of practice. 

First we start with libraries, as we always do. 

```{r}
library(tidyverse)
library(rvest)
library(janitor)
```

Now, let's run the code we wrote in the last chapter, to get a tidy list of NAICS sector codes and names from [https://www.bls.gov/ces/naics/]("https://www.bls.gov/ces/naics/").


```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

We'll use this table to help us get to our end goal: a single dataframe with the number of employees in each industry sector. 

It will look like this when we're done.  

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1a.png"))
```

Unfortunately, that information doesn't exist in a single tidy table on a single page we can scrape all at once.  We're going to have to scrape it from lots of different pages, and build it ourselves. 

Let's next take a look at the web page that has detailed employment information for one of our sectors, 22, Mining, Quarrying, and Oil and Gas Extraction.  

We can find it here: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm). 

A few scrolls down the page, there's a table that has employee statistics.  

The table is called "Employment and Unemployment". There's a row in the tabled for "Employment, all employees (seasonally adjusted)".  And in that row, there's a value for the number of employees -- in thousands -- in June 2021.  

The table shows that for the mining sector, it was 538.6  -- or 538,600 -- in June 2021.  That's the value we want to ingest in R. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1.png"))
```

 
We don't just want it for mining.  We want it for all sectors!  

But we'll start by writing code just to get it from this one sector page, then modify that code to get it from every sector's page

First, let's define the URL of the page we want to get the information from. 

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

```

Next, let's read in the html of that page, and store it as an object called employment_info.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html
employment_info <- url %>%
  read_html()  

# Display it so we can see what it looks like
employment_info

```

Now, let's set to picking out the information we need from the raw html. 

We can use the web inspector in our web browser (Chrome) to figure out where the table is located. 

Go to the web page and right click on the word "Data Series" in the table, then pick "inspect" to pull up the menu.

Notice two things.  First, all of this information is contained in a proper html `<table>`.  And that table has an id property of "iag22emp1". Designers use these IDs to help style the page, to target certain elements with CSS. We can use it to scrape.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest2.png"))
```
Recall that in the last chapter, when we used the html_table() function, it pulled in every single table on the page, six in total.  

Here, we can use that id property to pick out just the table we want, and leave all the others behind. 
We do that with a new function from rvest called html_element(), employing a bit of information about that element stored in what's called the  [xpath](https://en.wikipedia.org/wiki/XPath). Xpath is a query language that helps us write programs that target specific parts of web pages.  

The syntax is a little unwieldy, I know. 

But essentially what the html_element function says is "find the html element that has an id of iag22emp1, using the xpath method, and get rid of all other elements". 

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag22emp1"]')

# Display it so we can see what it looks like
employment_info
```

We've now isolated the table on the page that contains the information we need, and gotten rid of everything else. 

From here, we can use the html_tables() function to transform it from messy html code to a proper dataframe. 

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag22emp1"]') %>%
  html_table() 

# Display it so we can see what it looks like
employment_info
```

Now we have a proper dataframe of 6 rows and 6 columns.  

It has much more information than we need, so let's clean it up to isolate only the "Employment, all employees (seasonally adjusted)" value for June 2021. 

Use clean_names() to standardize the column names, use slice() to keep only the second row, and use select() to keep two columns data_series and jul_2021. 

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table() 

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, jul_2021)

# Display it so we can see what it looks like
employment_info
```

Okay, so we've successfully obtained the employment numbers for one of our sectors. That's great. 

But remember our original charge: to get a table with employment numbers for ALL sectors, not just one. 

This is a little tricky, because, remember, the information for each sector is on a different page! 

The info for mining is on this page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm). 

The info for construction is on this page: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm). 

We have 20 sectors to get through.  

We could get the info we need by copying the codeblock we just wrote 20 times, and change the url at the top each time.  

But that's not a great approach.  

What if we needed to change the code? We'd need to change it 20 times! 

In programming, there's a principle called "DRY" which stands for "Don't Repeat Yourself".  If you find yourself copying the same code over and over again, with minor changes, it's better to find a way to avoid that. 

## Using for loops

Fortunately, there's a programming paradigm called "iteration" that is helpful here, using a method called a "for loop". 

Every programming language has its own version of a "for loop", and R is no different. 

A "for loop" says: "let's take a list of things, and do the same thing to each item on that list."   

Let's look at a very simple example to help illustrate the values of for loops. 

We're going to write code to print out 10 industry sectors.  

First, let's do it the repetitive way.  We're writing the same print function over and over, just changing the sector name each time. 
```{r}

print("Agriculture, Forestry, Fishing and Hunting")
print("Mining, Quarrying, and Oil and Gas Extraction")
print("Utilities")
print("Construction")
print("Manufacturing")
print("Wholesale Trade")
print("Retail Trade")
print("Transportation and Warehousing")
print("Information")
print("Finance and Insurance")

```

We repeated print() 10 times, with minor modifications each time.  Lots of repetition, which we seek to avoid if possible. 

Now let's look at how we might do that a little more efficiently with a "for loop." 

First let's make a list of sectors, and save it as an object called "list_of_sectors." The c() function tells R that we're making a list.

```{r}
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")
  
```

And now let's write a "for loop" to print out sector on that list. 

```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# Make a for loop and run it
for (sector in list_of_sectors) {
  print(sector)
}

  
```

That's many fewer lines of code.  Let's break down what we just saw, starting with for `(sector in list_of sectors)`. 

The information inside the parentheses tells R what list to use -- list_of_sectors -- and how to identify list elements later on -- sector. 

It's important that the thing on the right side of "in" use the exact name of the list we want to loop through -- in this case "list_of_sectors". 

If we try to feed it something different -- say "sector_list" -- it won't work, because our actual list is called something else -- "list_of_sectors". This code throws an error. 

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that refers to a list that doesn't exist!  
for (sector in sector_list) {
  print(sector)
}

  
```

The name on the left side of "in" -- the word we're assigning to represent each element -- is totally arbitrary.  

We could use any character string, even something simple like "x".  

What matters is that we use the same character string inside of the curly braces {}, the section of the "for loop" that tells R what to do to each element -- in this case, print it out.    

To illustrate this, note that the code works just fine if we change it to say this:

```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop with x that stands in for each element in our list, instead of sector 
for (x in list_of_sectors) {
  print(x)
}

  
```

But it does NOT work if we call each element one thing -- x -- in the first line of our "for loop", and use a different name to refer to it inside of the curly braces.

In this code below, it has no idea what we mean by "sector_name", because we haven't defined that anywhere.  

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that includes instructions that refer to a variable that doesn't exist.
for (x in list_of_sectors) {
  print(sector_name)
}

  
```

We can also write for loops to iterate over a range of numbers, instead of a list of characters.  The syntax is a little different. 

The code below says: "for each number in a range of numbers from 1 to 10, print the number."  

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (number in 1:10) {
  print(number)
}
```

Here's a minor variation on that approach that we'll make use of below.  

Instead of giving the for loop an explicit number range, like 1:10, we can tell it to use 1 to "the number of rows in a dataframe" as our list of things to loop through. 

Remember the naics_industry dataframe we loaded first? It has 20 rows. 

```{r}

naics_industry

```

We can use that information in our for loop by using the nrow() function, which calculates the number of rows in a dataframe.  Here's a quick demonstration of how that works. 

```{r}
nrow(naics_industry)
```

To put it all together, the code below says "make a list of numbers that starts at 1 and ends at the number of rows in the naics_industry dataframe (which is 20), then print out each of these numbers." 

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (row_number in 1:nrow(naics_industry)) {
  print(row_number)
}
```

These were basic examples of how "for loops" work.  Next, we'll learn to apply "for loops" to efficentily extract information from multiple web pages. 

## Looping and rvest

First, let's look at the codeblock we wrote earlier to extract the number of employees in the mining sector.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table() 

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, jul_2021)

# Display it so we can see what it looks like
employment_info
```
This contains all the steps we needed to extract the information from one sector page. We're now going to modify this function so we can use it to extract information from each sector page, writing code that keeps us from repeating ourselves too much.  
First, we need to build a list of URLs to loop through in a "for loop." We can do that using the dataframe we made in the last chapter. 

```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

This gives us the sector code and name for each industry. 

Now let's have a look at the URLs for a few of the pages we want to grab data from. 

* Mining, Quarrying, and Oil and Gas Extraction: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm). 
* Utilities: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).
* Construction: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm)

Notice a pattern?

They all start with "https://www.bls.gov/iag/tgs/iag".  The next bit of information is different for each one; with the two-digit sector code for each sector.  The remainder is identical in all three links, ".htm". 

Because they're all the same, we can use the information in the dataframe we just loaded to make all the URLs we need. 

We're going to use mutate() and paste0() to concatenate (mash together) the things that stay constant in every url (the beginning and end) with the things that are different (the sector number, stored in the column called sector).

```{r}

# Make a column with URL for each sector. 
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))

# Display it
naics_industry
```

While we're at it, we're going to use the same method to programatically build the "xpath" for the table on each sector page.  

Recall that when we wrote our function that got information from just the mining page, the xpath targeted an element with an ID of "iag21emp1".  Why 21? That's the sector code for mining.  

If we look for that exact element ID on other sector pages, we won't find it! That's because it's different for each page. 

On the Utilities page (sector code 22), the ID for the table we want is "iag22emp1".  On the Construction page (sector code 23), it's "iag23emp1". We can also build this programatically, because it follows a predictable pattern.

```{r}

# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1"))

# Display it
naics_industry
```

Lastly, we're going to use filter to remove the "Public Administration" sector, because there's no page for it. We'll have to get that information some other way.

```{r}

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
  filter(description != "Public Administration")
# Display it
naics_industry
```

We're left with a dataframe of 19 rows and 4 columns. It now contains everything we need. 

Next, we'll construct a "for loop" to extract the info we need from each page. We're going to build it up step-by-step, beginning with the the basic elements of our "for loop". 

The codeblock below says: "Make a list with the row numbers from 1 to the number of rows in our naics_industry dataframe (which is 19). Then, for each element of that list (1, 2, 3, 4, 5 and so on up to 19), use slice() to keep only the one row that matches that number and save this newly created dataframe as each_row_df. Print out the dataframe. Then go to the next element on the list and do the same thing.  Keep doing that until we hit number 19, then stop."   

We get 19 dataframes, each with one row, one for each sector. 

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(each_row_df)
      
}
```

We're almost to the part where we can go out and fetch the html we need. Before we do that, let's store as part of our loop an object called "url", which contains the URL of the page for each sector. 

The syntax with the dollar sign is a little funky, but "each_row_df$sector_url" says "from the each_row_df dataframe, grab the information in the sector_url column." Because the column has only one row, there's one value. 

We're going to do something simliar with the xpath for our employment table by using the information in the sector_xpath_id column. 

That code also looks a little unwieldly.  Recall that the xpath for the mining industry was `'//*[@id="iag22emp1"]'`.  

In the code below, we're building the xpath dynamically by pasting together the parts that stay the same for each xpath -- `'//*[@id="'` and `'"]'` -- and the parts that change for each sector, pulled from the xpath_sector_id column. 

To see how this is working, we're going to edit our print statement at the end a bit, printing the row_number and the dynamically created url and xpath. 
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))
      
}
```

Armed with the URL and xpath for each sector web page, we can now go out and get the employment table for each sector. 

We'll read in the html from the url we just stored; extract the table that has the xpath ID we just created; and then transform the html table code into a proper dataframe. 

The dataframe is hidden inside  a nested list, which we'll have to extract in the next step. 

So, when you run this code, it will print out 19 dataframes inside of nested lists, each containing one dataframe.

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table() 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)
    
      
}
```
In this next step, we use employment_info <- employment_info[[1]]  to extract each dataframe from the nested list. Then we'll tidy up the dataframe a bit. We'll use the get rid of all the information we don't need in the table, by using slice() to keep only the second row. We'll also standardize the column names with clean_names(). 

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table() 
    
    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; 
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)
    
      
}
```
We now have 19 dataframes, each containing one row each and two columns, one of which is the employment number for a given sector for jul_2021. But we're missing information about what industry sector these employment numbers represent. 

We can add that back in by using bind_cols() to reconnect the each_row_df, which contains the sector code and the sector name.   
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table() 
    
    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>% 
      bind_cols(each_row_df) 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)
    
      
}
```
Then we'll do a little bit of cleaning. 

Let's use parse_number() to remove the comma from the jul_2021 number and convert it from a character to number. We'll use rename() to make the jul_2021 column name a little more descriptive. And then we'll use select() to keep only the columns we want to keep -- the sector number, the sector name, and the jul_2021 employment number.

```{r}

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table() 
    
    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jul_2021 column into a proper number, and rename it.  Then select only three columns we need. 
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>% 
      bind_cols(each_row_df) %>%
      mutate(jul_2021 = parse_number(jul_2021)) %>%
      rename(jul_2021_employees = jul_2021) %>%
      select(sector,description,jul_2021_employees) 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)
    
    
}

```


We're getting very close to the finished table we showed at the beginning.  

But right now, each bit of sector information is separated between 19 different dataframes.  

We want them in one dataframe.  

We can fix this by creating an empty dataframe called "employment_by_sector_all" using tibble(), placing it before our "for loop". 

And inside our "for loop" at the end, we'll bind each employment_info dataframe to the newly created empty dataframe.  

```{r}

# Create an empty dataframe to hold results 
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {
    
    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number) 
      
    # Define url of page to get
    url <- each_row_df$sector_url
    
    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table() 
    
    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jul_2021 column into a proper number, and rename it.  Then select only three columns we need. 
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>% 
      bind_cols(each_row_df) %>%
      mutate(jul_2021 = parse_number(jul_2021)) %>%
      rename(jul_2021_employees = jul_2021) %>%
      select(sector,description,jul_2021_employees) 
    
    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all %>%
      bind_rows(employment_info)
    
}

# Display the completed dataframe
employment_by_sector_all
```

Ta da! The end result is a nice tidy dataframe with the number of employees in June 2021 for each sector. 

It's always a good idea to spot check the results, especially any values that look suspciously high or low. 

The value for "Agriculture, Forestry, Fishing and Hunting" seems suspiciously low, compared with the other values.  

Let's figure out why.  

Here's the table on the mining sector page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm)

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest3.png"))
```
And here's the table for the agriculture sector. 

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest4.png"))
```
Unlike mining -- and every other sector page (I checked each page) -- the agriculture page is structured differently.  

In the second row of this table, it has the unemployment rate. Nowhere on the page can we find information on the number of employees.  We would need to do additional research to track down a valid number if we plan on using this table, but for now we're going to replace it with an NA using na_if.

```{r}
# remove the suspicious value for agriculture. 
employment_by_sector_all <- employment_by_sector_all %>%
  mutate(jul_2021_employees = na_if(jul_2021_employees,7.5))

# display it
employment_by_sector_all
```

And we're done.  

A note about advanced scraping -- every site is different. Every time you want to scrape a site, you'll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it. 
